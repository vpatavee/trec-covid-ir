{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reranking using SciBERT\n",
    "\n",
    "This Notebook is to rerun the Anserini Baseline using SciBERT.\n",
    "\n",
    "\n",
    "## Prerequisite\n",
    "\n",
    "### CORD-19 Dataset (Rev 2020-06-19)\n",
    "\n",
    "Download [here](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases.html)\n",
    "\n",
    "### Trec Eval\n",
    "\n",
    "\n",
    "A program used for IR evaluation. Please have [trec_eval](https://github.com/usnistgov/trec_eval) installed. Alternatively, use [Python](https://github.com/cvangysel/pytrec_eval) version.\n",
    "\n",
    "Install original trec_eval:\n",
    "1. Go to parent directory of this repo\n",
    "2. git clone https://github.com/usnistgov/trec_eval.git\n",
    "3. make\n",
    "\n",
    "### Anserini Baseline Run\n",
    "\n",
    "Get Round 4 Anserini Baseline from https://github.com/castorini/anserini/blob/master/docs/experiments-covid.md\n",
    "```\n",
    "wget -P ../tmp/ https://www.dropbox.com/s/lbgevu4wiztd9e4/anserini.covid-r4.abstract.qdel.bm25.txt\n",
    "```\n",
    "    \n",
    "### Topics\n",
    "\n",
    "The list of queries to be used against CORD-19 dataset.\n",
    "\n",
    "```\n",
    "wget -P ../tmp/ https://ir.nist.gov/covidSubmit/data/topics-rnd4.xml\n",
    "```\n",
    "\n",
    "### Qrels\n",
    "\n",
    "The human evaluation of the retrieved document against topics. Used to evaluate the performance of IR system.\n",
    "\n",
    "```\n",
    "\n",
    "wget -P ../tmp/ https://ir.nist.gov/covidSubmit/data/qrels-covid_d4_j0.5-4.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy\n",
    "from tqdm import tqdm\n",
    "from transformers import *\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from lib.utils import read_topics, get_abstract\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "PATH_TO_CORD_19_DATA = \"../../CORD-19/2020-06-19/\"\n",
    "PATH_TO_ANSERINI_RUN = \"anserini.covid-r4.abstract.qdel.bm25.txt\"\n",
    "PATH_TO_TOPICS = \"../tmp/topics-rnd4.xml\"\n",
    "PATH_TO_TREC = \"../../trec_eval/trec_eval\"\n",
    "PATH_TO_QRELS = \"../tmp/qrels-covid_d4_j0.5-4.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at Anserini baseline. We will see if SciBERT can imporve\n",
    "\n",
    "ret = os.system(PATH_TO_TREC + \" -c -m all_trec \" + PATH_TO_QRELS + \" \" + PATH_TO_ANSERINI_RUN + \" > ../tmp/out.txt\")\n",
    "\n",
    "if ret == 0:\n",
    "    with open(\"../tmp/out.txt\", \"r\") as f:\n",
    "        print(f.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anserini Baseline Before Reranking**\n",
    "```\n",
    "runid                 \tall\tanserini.covid-r4.abstract.qdel.bm25.txt\n",
    "num_q                 \tall\t45\n",
    "num_ret               \tall\t447306\n",
    "num_rel               \tall\t15765\n",
    "num_rel_ret           \tall\t12201\n",
    "map                   \tall\t0.2994\n",
    "gm_map                \tall\t0.2325\n",
    "Rprec                 \tall\t0.3484\n",
    "bpref                 \tall\t0.5407\n",
    "recip_rank            \tall\t0.8660\n",
    "iprec_at_recall_0.00  \tall\t0.9165\n",
    "iprec_at_recall_0.10  \tall\t0.6471\n",
    "iprec_at_recall_0.20  \tall\t0.5301\n",
    "iprec_at_recall_0.30  \tall\t0.4325\n",
    "iprec_at_recall_0.40  \tall\t0.3391\n",
    "iprec_at_recall_0.50  \tall\t0.2526\n",
    "iprec_at_recall_0.60  \tall\t0.1871\n",
    "iprec_at_recall_0.70  \tall\t0.1276\n",
    "iprec_at_recall_0.80  \tall\t0.0689\n",
    "iprec_at_recall_0.90  \tall\t0.0258\n",
    "iprec_at_recall_1.00  \tall\t0.0018\n",
    "P_5                   \tall\t0.7911\n",
    "P_10                  \tall\t0.7689\n",
    "P_15                  \tall\t0.7407\n",
    "P_20                  \tall\t0.7189\n",
    "P_30                  \tall\t0.6911\n",
    "P_100                 \tall\t0.5398\n",
    "P_200                 \tall\t0.4162\n",
    "P_500                 \tall\t0.2657\n",
    "P_1000                \tall\t0.1713\n",
    "recall_5              \tall\t0.0157\n",
    "recall_10             \tall\t0.0294\n",
    "recall_15             \tall\t0.0426\n",
    "recall_20             \tall\t0.0549\n",
    "recall_30             \tall\t0.0788\n",
    "recall_100            \tall\t0.1913\n",
    "recall_200            \tall\t0.2856\n",
    "recall_500            \tall\t0.4238\n",
    "recall_1000           \tall\t0.5233\n",
    "infAP                 \tall\t0.2994\n",
    "gm_bpref              \tall\t0.4535\n",
    "Rprec_mult_0.20       \tall\t0.6115\n",
    "Rprec_mult_0.40       \tall\t0.5108\n",
    "Rprec_mult_0.60       \tall\t0.4363\n",
    "Rprec_mult_0.80       \tall\t0.3878\n",
    "Rprec_mult_1.00       \tall\t0.3484\n",
    "Rprec_mult_1.20       \tall\t0.3164\n",
    "Rprec_mult_1.40       \tall\t0.2899\n",
    "Rprec_mult_1.60       \tall\t0.2675\n",
    "Rprec_mult_1.80       \tall\t0.2482\n",
    "Rprec_mult_2.00       \tall\t0.2325\n",
    "utility               \tall\t-9397.8667\n",
    "11pt_avg              \tall\t0.3208\n",
    "binG                  \tall\t0.1522\n",
    "G                     \tall\t0.1313\n",
    "ndcg                  \tall\t0.6607\n",
    "ndcg_rel              \tall\t0.5722\n",
    "Rndcg                 \tall\t0.4991\n",
    "ndcg_cut_5            \tall\t0.7299\n",
    "ndcg_cut_10           \tall\t0.7081\n",
    "ndcg_cut_15           \tall\t0.6794\n",
    "ndcg_cut_20           \tall\t0.6650\n",
    "ndcg_cut_30           \tall\t0.6403\n",
    "ndcg_cut_100          \tall\t0.5201\n",
    "ndcg_cut_200          \tall\t0.4570\n",
    "ndcg_cut_500          \tall\t0.4650\n",
    "ndcg_cut_1000         \tall\t0.5236\n",
    "map_cut_5             \tall\t0.0151\n",
    "map_cut_10            \tall\t0.0270\n",
    "map_cut_15            \tall\t0.0380\n",
    "map_cut_20            \tall\t0.0483\n",
    "map_cut_30            \tall\t0.0669\n",
    "map_cut_100           \tall\t0.1449\n",
    "map_cut_200           \tall\t0.1963\n",
    "map_cut_500           \tall\t0.2499\n",
    "map_cut_1000          \tall\t0.2746\n",
    "relative_P_5          \tall\t0.7911\n",
    "relative_P_10         \tall\t0.7689\n",
    "relative_P_15         \tall\t0.7407\n",
    "relative_P_20         \tall\t0.7189\n",
    "relative_P_30         \tall\t0.6911\n",
    "relative_P_100        \tall\t0.5455\n",
    "relative_P_200        \tall\t0.4541\n",
    "relative_P_500        \tall\t0.4353\n",
    "relative_P_1000       \tall\t0.5233\n",
    "success_1             \tall\t0.8000\n",
    "success_5             \tall\t0.9333\n",
    "success_10            \tall\t1.0000\n",
    "set_P                 \tall\t0.0273\n",
    "set_relative_P        \tall\t0.7831\n",
    "set_recall            \tall\t0.7831\n",
    "set_map               \tall\t0.0218\n",
    "set_F                 \tall\t0.0523\n",
    "num_nonrel_judged_ret \tall\t13706\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Anserini Baseline\n",
    "\n",
    "with open(PATH_TO_ANSERINI_RUN, \"r\") as f:\n",
    "    anserini_run = f.readlines()\n",
    "    \n",
    "set_of_top_k_uid_all_topics = set() # We won't get all abstracts because there are too many.\n",
    "\n",
    "for line in anserini_run:\n",
    "    qid, _, uid, rank, _, _ = line.strip().split()\n",
    "    if int(rank) <= 200:\n",
    "        set_of_top_k_uid_all_topics.add(uid)\n",
    "\n",
    "# Read topics\n",
    "\n",
    "topics = read_topics(PATH_TO_TOPICS)\n",
    "\n",
    "# Read Abstract\n",
    "\n",
    "abstracts_dict = get_abstract(PATH_TO_CORD_19_DATA, set_of_top_k_uid_all_topics)\n",
    "\n",
    "\n",
    "# load SciBert (alternative: monologg/biobert_v1.1_pubmed)\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', do_lower_case=False)\n",
    "model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reranking using SciBERT + Words-level Matches\n",
    "\n",
    "\n",
    "What we are doing is - given Anserini Run on abstract, use SciBERT to rerank first K abstracts, and evaluate using TREC metric as usual. \n",
    "\n",
    "In short, we use SciBERT to encode the abstracts and queries into vectors of size [ num_of_tokens, 768]. If the sentence is longer than maximum size of BERT, use slicing windows. We use the vectors of the last hidden states of BERT model.\n",
    "\n",
    "\n",
    "Once both abstracts and queries are encoded, we do token level cosine similarity to score the relevant of paragraph and query with the highly matched words.\n",
    "\n",
    "This approach is inspired by [Notebook](https://github.com/castorini/anserini-notebooks/blob/master/Pyserini%2BSciBERT_on_COVID_19_Demo.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 510\n",
    "\n",
    "def extract_scibert(text, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Encode text to vectors\n",
    "    @text string to be encoded\n",
    "    @tokenizer BertTokenizer object\n",
    "    @model BertModel object\n",
    "    @return tensor of size [num_tokens, 768] (last hidden state of BERT)\n",
    "    \"\"\"\n",
    "    \n",
    "    text_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])\n",
    "    text_words = tokenizer.convert_ids_to_tokens(text_ids[0])[1:-1]\n",
    "\n",
    "    n_chunks = int(numpy.ceil(float(text_ids.size(1))/CHUNK_SIZE))\n",
    "    states = []\n",
    "    \n",
    "    for ci in range(n_chunks):\n",
    "        text_ids_ = text_ids[0, 1+ci*CHUNK_SIZE:1+(ci+1)*CHUNK_SIZE]  \n",
    "        text_ids_ = torch.cat([text_ids[0, 0].unsqueeze(0), text_ids_])\n",
    "        if text_ids_[-1] != text_ids[0, -1]: # their code is wrong\n",
    "\n",
    "            text_ids_ = torch.cat([text_ids_, text_ids[0,-1].unsqueeze(0)])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state = model(text_ids_.unsqueeze(0))[0] # last hidden states\n",
    "            state = state[:, 1:-1, :]\n",
    "        states.append(state)\n",
    "\n",
    "    state = torch.cat(states, axis=1)\n",
    "    # return text_ids, text_words, state[0]\n",
    "    return state[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_abstract_query_narrative_and_save(topics, abstracts_dict, extract_scibert, tokenizer, model, path_to_output, **kwargs):\n",
    "    \"\"\"\n",
    "    Encode topics and abstracts using given encoding function, save it to path_to_output\n",
    "    @topics\n",
    "    @abstracts_dict\n",
    "    @extract_scibert BERT encoding function, \n",
    "        input: text, tokenizer, model **kwargs, \n",
    "        output  tensor of size [num, 768]\n",
    "    @tokenizer to pass to extract_scibert\n",
    "    @model to pass to extract_scibert\n",
    "    \"\"\"\n",
    "    \n",
    "    # encode abstract\n",
    "    encoded_abstract = dict()\n",
    "\n",
    "    for k, text in tqdm(abstracts_dict.items()):\n",
    "        encoded_abstract[text] = extract_scibert(text, tokenizer, model, **kwargs)\n",
    "\n",
    "    # encode topics\n",
    "    encoded_queries = dict()\n",
    "    encoded_naratives = dict()\n",
    "\n",
    "    for topic in topics:\n",
    "        encoded_queries[topic[\"query\"]] = extract_scibert(topic[\"query\"], tokenizer, model)\n",
    "        encoded_naratives[topic[\"narrative\"]] = extract_scibert(topic[\"narrative\"], tokenizer, model)\n",
    "\n",
    "    # save for future use\n",
    "    bert_vectors = {\n",
    "            \"abstract\": encoded_abstract, \n",
    "            \"query\": encoded_queries, \n",
    "            \"narrative\": encoded_naratives\n",
    "    }\n",
    "\n",
    "    with open(\"../tmp/\" + path_to_output, \"wb\") as f:\n",
    "        pickle.dump(bert_vectors, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_vectors_output = \"scibert.pkl\"\n",
    "encode_abstract_query_narrative_and_save(\n",
    "    topics, \n",
    "    abstracts_dict, \n",
    "    extract_scibert, \n",
    "    tokenizer, \n",
    "    model, \n",
    "    encoded_vectors_output)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "with open(\"../tmp/scibert.pkl\", \"rb\") as f:\n",
    "    bert_vectors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_match(state1, state2):\n",
    "    state1 = state1 / torch.sqrt((state1 ** 2).sum(1, keepdims=True))\n",
    "    state2 = state2 / torch.sqrt((state2 ** 2).sum(1, keepdims=True))\n",
    "    sim = (state1.unsqueeze(1) * state2.unsqueeze(0)).sum(-1)\n",
    "    return sim\n",
    "\n",
    "def rerank(topics, anserini_run, abstracts_dict, top_k, path_to_reranked_run, topics_field, bert_vectors):\n",
    "    \"\"\"\n",
    "    Rerank the original run and save the reranked run in path_to_reranked_run\n",
    "    @topics list of dict where dict represents each topic\n",
    "    [\n",
    "        {\n",
    "            'number': '1', \n",
    "            'query': 'coronavirus origin', \n",
    "            'question': 'what is the origin of COVID-19',\n",
    "            'narrative': \"seeking range of information about ...\"\n",
    "         },\n",
    "         ...\n",
    "    ]\n",
    "    @abstracts_dict dict where key=uid and value=abstract\n",
    "    @anserini_run list of string run in anserini_run\n",
    "    @top_k to rerank, the rest will remain same\n",
    "    @path_to_reranked_run path to save the reranked run\n",
    "    @topics_field query or narrative\n",
    "    @bert_vectors dict k=query,abstract,narrative v=vectors\n",
    "    \"\"\"\n",
    "    rerank = defaultdict(list)  # first k hits\n",
    "    keeprank = defaultdict(list) # k+1 to 1000 hits\n",
    "    query_dict = {e[\"number\"]: e[topics_field] for e in topics}  # choose whether to use query or narrative\n",
    "    encoded_queries = bert_vectors[topics_field] # choose whether to use query or narrative\n",
    "    encoded_abstract = bert_vectors[\"abstract\"]\n",
    "\n",
    "    # calculate similarity\n",
    "    for line in anserini_run:\n",
    "        qid, _, uid, j, score, _ = line.strip().split()\n",
    "        if len(rerank[qid]) < top_k:\n",
    "            if not abstracts_dict[uid]:\n",
    "                continue # Some uid don't have abstract. But why they show up in Anserini run?\n",
    "\n",
    "            _, _, enc_abs = encoded_abstract[abstracts_dict[uid]]\n",
    "            _, _, enc_query = encoded_queries[query_dict[qid]]\n",
    "            sim = cross_match(enc_query, enc_abs)\n",
    "\n",
    "            rel_score = torch.max(sim).item()\n",
    "            rerank[qid].append([uid, rel_score])\n",
    "\n",
    "        elif len(rerank[qid]) >= top_k and len(keeprank[qid]) < 1000 - top_k: \n",
    "            keeprank[qid].append([uid, score, j])\n",
    "\n",
    "\n",
    "    # create reranked run and save to path_to_reranked_run\n",
    "    template = \"{} Q0 {} {} {} anserini_scibert\"\n",
    "    run = list()\n",
    "\n",
    "    for qid in rerank:\n",
    "        rank = 1\n",
    "        for uid, score in sorted(rerank[qid], key=lambda x:-x[1]):\n",
    "            run.append(template.format(qid, uid, rank, score + 10))\n",
    "            rank += 1\n",
    "\n",
    "        for uid, score, j in keeprank[qid]:\n",
    "            run.append(template.format(qid, uid, rank, score))\n",
    "            rank += 1\n",
    "            \n",
    "        assert rank == 1001 # if no bugs, each topic will have at most 1000 uid (can be less if original run has less)\n",
    "\n",
    "    with open(\"../tmp/\" + path_to_reranked_run, \"w\") as f:\n",
    "        f.write(\"\\n\".join(run))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_reranked_run = \"scibert_sim_rerank.txt\"\n",
    "topics_field = \"narrative\" # query or narrative\n",
    "rerank(topics, anserini_run, abstracts_dict, 100, path_to_reranked_run, topics_field, bert_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ret = os.system(PATH_TO_TREC + \" -c -m all_trec ../tmp/qrels-covid_d4_j0.5-4.txt  ../tmp/scibert_sim_rerank_on_narative.txt > ../tmp/out.txt\")\n",
    "if ret == 0:\n",
    "    with open(\"../tmp/out.txt\", \"r\") as f:\n",
    "        print(f.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result after reranking**\n",
    "\n",
    "\n",
    "It turns out that this approach doesn't improve the reranking.\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "runid                 \tall\tanserini_scibert\n",
    "num_q                 \tall\t45\n",
    "num_ret               \tall\t44955\n",
    "num_rel               \tall\t15765\n",
    "num_rel_ret           \tall\t7678\n",
    "map                   \tall\t0.1947\n",
    "gm_map                \tall\t0.1564\n",
    "Rprec                 \tall\t0.2981\n",
    "bpref                 \tall\t0.4178\n",
    "recip_rank            \tall\t0.7022\n",
    "iprec_at_recall_0.00  \tall\t0.8272\n",
    "iprec_at_recall_0.10  \tall\t0.4782\n",
    "iprec_at_recall_0.20  \tall\t0.3947\n",
    "iprec_at_recall_0.30  \tall\t0.3183\n",
    "iprec_at_recall_0.40  \tall\t0.2316\n",
    "iprec_at_recall_0.50  \tall\t0.1332\n",
    "iprec_at_recall_0.60  \tall\t0.0942\n",
    "iprec_at_recall_0.70  \tall\t0.0484\n",
    "iprec_at_recall_0.80  \tall\t0.0317\n",
    "iprec_at_recall_0.90  \tall\t0.0053\n",
    "iprec_at_recall_1.00  \tall\t0.0000\n",
    "P_5                   \tall\t0.5244\n",
    "P_10                  \tall\t0.5378\n",
    "P_15                  \tall\t0.5126\n",
    "P_20                  \tall\t0.4933\n",
    "P_30                  \tall\t0.4778\n",
    "P_100                 \tall\t0.4300\n",
    "P_200                 \tall\t0.3614\n",
    "P_500                 \tall\t0.2406\n",
    "P_1000                \tall\t0.1706\n",
    "recall_5              \tall\t0.0091\n",
    "recall_10             \tall\t0.0177\n",
    "recall_15             \tall\t0.0258\n",
    "recall_20             \tall\t0.0333\n",
    "recall_30             \tall\t0.0473\n",
    "recall_100            \tall\t0.1445\n",
    "recall_200            \tall\t0.2385\n",
    "recall_500            \tall\t0.3819\n",
    "recall_1000           \tall\t0.5172\n",
    "infAP                 \tall\t0.1947\n",
    "gm_bpref              \tall\t0.3567\n",
    "Rprec_mult_0.20       \tall\t0.4542\n",
    "Rprec_mult_0.40       \tall\t0.3903\n",
    "Rprec_mult_0.60       \tall\t0.3560\n",
    "Rprec_mult_0.80       \tall\t0.3260\n",
    "Rprec_mult_1.00       \tall\t0.2981\n",
    "Rprec_mult_1.20       \tall\t0.2731\n",
    "Rprec_mult_1.40       \tall\t0.2512\n",
    "Rprec_mult_1.60       \tall\t0.2327\n",
    "Rprec_mult_1.80       \tall\t0.2173\n",
    "Rprec_mult_2.00       \tall\t0.2044\n",
    "utility               \tall\t-657.7556\n",
    "11pt_avg              \tall\t0.2330\n",
    "binG                  \tall\t0.0900\n",
    "G                     \tall\t0.0810\n",
    "ndcg                  \tall\t0.4718\n",
    "ndcg_rel              \tall\t0.4235\n",
    "Rndcg                 \tall\t0.3752\n",
    "ndcg_cut_5            \tall\t0.4589\n",
    "ndcg_cut_10           \tall\t0.4637\n",
    "ndcg_cut_15           \tall\t0.4468\n",
    "ndcg_cut_20           \tall\t0.4326\n",
    "ndcg_cut_30           \tall\t0.4216\n",
    "ndcg_cut_100          \tall\t0.3865\n",
    "ndcg_cut_200          \tall\t0.3628\n",
    "ndcg_cut_500          \tall\t0.3883\n",
    "ndcg_cut_1000         \tall\t0.4718\n",
    "map_cut_5             \tall\t0.0072\n",
    "map_cut_10            \tall\t0.0128\n",
    "map_cut_15            \tall\t0.0172\n",
    "map_cut_20            \tall\t0.0214\n",
    "map_cut_30            \tall\t0.0293\n",
    "map_cut_100           \tall\t0.0768\n",
    "map_cut_200           \tall\t0.1181\n",
    "map_cut_500           \tall\t0.1640\n",
    "map_cut_1000          \tall\t0.1947\n",
    "relative_P_5          \tall\t0.5244\n",
    "relative_P_10         \tall\t0.5378\n",
    "relative_P_15         \tall\t0.5126\n",
    "relative_P_20         \tall\t0.4933\n",
    "relative_P_30         \tall\t0.4778\n",
    "relative_P_100        \tall\t0.4320\n",
    "relative_P_200        \tall\t0.3882\n",
    "relative_P_500        \tall\t0.3934\n",
    "relative_P_1000       \tall\t0.5172\n",
    "success_1             \tall\t0.5556\n",
    "success_5             \tall\t0.8667\n",
    "success_10            \tall\t0.9556\n",
    "set_P                 \tall\t0.1708\n",
    "set_relative_P        \tall\t0.5172\n",
    "set_recall            \tall\t0.5172\n",
    "set_map               \tall\t0.0937\n",
    "set_F                 \tall\t0.2449\n",
    "num_nonrel_judged_ret \tall\t5724\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reranking using SciBERT + Phrase-level Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_scibert_phrase(text, tokenizer, model, pool, window_size = 100, overlap_size = 20):\n",
    "    \"\"\"\n",
    "    Encode text to vectors\n",
    "    @text string to be encoded\n",
    "    @tokenizer BertTokenizer object\n",
    "    @model BertModel object\n",
    "    @pool \"cls\" or \"sum\". If class, use [CLS] vector. If sum, sum all token vectors except [CLS] and [SEP]. \n",
    "    see https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel\n",
    "    @return tensor of size [num_phrases, 768]\n",
    "    \"\"\"\n",
    "    \n",
    "    num_phrases = len(text.split()) // (window_size -  overlap_size) \n",
    "    \n",
    "    vectors = []\n",
    "    for batch in range(num_phrases):\n",
    "        start = batch * (window_size - overlap_size)\n",
    "        end = start + window_size\n",
    "        s = \" \".join(text.split()[start:end])\n",
    "\n",
    "        # convert to vectors\n",
    "        inputs = tokenizer(s, return_tensors=\"pt\")\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        if pool == \"cls\":\n",
    "            output = outputs[1]\n",
    "        elif pool == \"sum\":\n",
    "            output = outputs[0][0,1:-1,:].sum(axis=0).reshape(1,-1)\n",
    "        else:\n",
    "            print(\"Invalid Pool Method. Must be 'cls' or 'sum'.\")\n",
    "\n",
    "        vectors.append(output)\n",
    "    \n",
    "    output_vectors = torch.cat(vectors, axis=0)\n",
    "    return output_vectors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encode_abstract_query_narrative_and_save(\n",
    "    topics, \n",
    "    abstracts_dict, \n",
    "    extract_scibert_phrase, \n",
    "    tokenizer, \n",
    "    model, \n",
    "    \"scibert_phrase_cls_w_100_o_20.pkl\",\n",
    "    pool=\"cls\"\n",
    "\n",
    ")\n",
    "    \n",
    "    \n",
    "\n",
    "encode_abstract_query_narrative_and_save(\n",
    "    topics, \n",
    "    abstracts_dict, \n",
    "    extract_scibert_phrase, \n",
    "    tokenizer, \n",
    "    model, \n",
    "    \"scibert_phrase_sum_w_100_o_20.pkl\",\n",
    "    pool=\"sum\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reranking with BertForNextSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
