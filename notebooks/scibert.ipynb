{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reranking using SciBERT\n",
    "\n",
    "This Notebook is to rerun the Anserini Baseline using SciBERT.\n",
    "\n",
    "\n",
    "## Prerequisite\n",
    "\n",
    "### CORD-19 Dataset (Rev 2020-06-19)\n",
    "\n",
    "Download [here](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases.html)\n",
    "\n",
    "### Trec Eval\n",
    "\n",
    "\n",
    "A program used for IR evaluation. Please have [trec_eval](https://github.com/usnistgov/trec_eval) installed. Alternatively, use [Python](https://github.com/cvangysel/pytrec_eval) version.\n",
    "\n",
    "Install original trec_eval:\n",
    "1. Go to parent directory of this repo\n",
    "2. git clone https://github.com/usnistgov/trec_eval.git\n",
    "3. make\n",
    "\n",
    "### Anserini Baseline Run\n",
    "\n",
    "Get Round 4 Anserini Baseline from https://github.com/castorini/anserini/blob/master/docs/experiments-covid.md\n",
    "```\n",
    "wget -P ../tmp/ https://www.dropbox.com/s/lbgevu4wiztd9e4/anserini.covid-r4.abstract.qdel.bm25.txt\n",
    "```\n",
    "    \n",
    "### Topics\n",
    "\n",
    "The list of queries to be used against CORD-19 dataset.\n",
    "\n",
    "```\n",
    "wget -P ../tmp/ https://ir.nist.gov/covidSubmit/data/topics-rnd4.xml\n",
    "```\n",
    "\n",
    "### Qrels\n",
    "\n",
    "The human evaluation of the retrieved document against topics. Used to evaluate the performance of IR system.\n",
    "\n",
    "```\n",
    "\n",
    "wget -P ../tmp/ https://ir.nist.gov/covidSubmit/data/qrels-covid_d4_j0.5-4.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy\n",
    "from tqdm import tqdm\n",
    "from transformers import *\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from lib.utils import read_topics, get_abstract\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "PATH_TO_CORD_19_DATA = \"../../CORD-19/2020-06-19/\"\n",
    "PATH_TO_ANSERINI_RUN = \"anserini.covid-r4.abstract.qdel.bm25.txt\"\n",
    "PATH_TO_TOPICS = \"../tmp/topics-rnd4.xml\"\n",
    "PATH_TO_TREC = \"../../trec_eval/trec_eval\"\n",
    "PATH_TO_QRELS = \"../tmp/qrels-covid_d4_j0.5-4.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at Anserini baseline. We will see if SciBERT can imporve\n",
    "\n",
    "ret = os.system(PATH_TO_TREC + \" -c -m all_trec \" + PATH_TO_QRELS + \" \" + PATH_TO_ANSERINI_RUN + \" > ../tmp/out.txt\")\n",
    "\n",
    "if ret == 0:\n",
    "    with open(\"../tmp/out.txt\", \"r\") as f:\n",
    "        print(f.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anserini Baseline Before Reranking**\n",
    "```\n",
    "runid                 \tall\tanserini.covid-r4.abstract.qdel.bm25.txt\n",
    "num_q                 \tall\t45\n",
    "num_ret               \tall\t447306\n",
    "num_rel               \tall\t15765\n",
    "num_rel_ret           \tall\t12201\n",
    "map                   \tall\t0.2994\n",
    "gm_map                \tall\t0.2325\n",
    "Rprec                 \tall\t0.3484\n",
    "bpref                 \tall\t0.5407\n",
    "recip_rank            \tall\t0.8660\n",
    "iprec_at_recall_0.00  \tall\t0.9165\n",
    "iprec_at_recall_0.10  \tall\t0.6471\n",
    "iprec_at_recall_0.20  \tall\t0.5301\n",
    "iprec_at_recall_0.30  \tall\t0.4325\n",
    "iprec_at_recall_0.40  \tall\t0.3391\n",
    "iprec_at_recall_0.50  \tall\t0.2526\n",
    "iprec_at_recall_0.60  \tall\t0.1871\n",
    "iprec_at_recall_0.70  \tall\t0.1276\n",
    "iprec_at_recall_0.80  \tall\t0.0689\n",
    "iprec_at_recall_0.90  \tall\t0.0258\n",
    "iprec_at_recall_1.00  \tall\t0.0018\n",
    "P_5                   \tall\t0.7911\n",
    "P_10                  \tall\t0.7689\n",
    "P_15                  \tall\t0.7407\n",
    "P_20                  \tall\t0.7189\n",
    "P_30                  \tall\t0.6911\n",
    "P_100                 \tall\t0.5398\n",
    "P_200                 \tall\t0.4162\n",
    "P_500                 \tall\t0.2657\n",
    "P_1000                \tall\t0.1713\n",
    "recall_5              \tall\t0.0157\n",
    "recall_10             \tall\t0.0294\n",
    "recall_15             \tall\t0.0426\n",
    "recall_20             \tall\t0.0549\n",
    "recall_30             \tall\t0.0788\n",
    "recall_100            \tall\t0.1913\n",
    "recall_200            \tall\t0.2856\n",
    "recall_500            \tall\t0.4238\n",
    "recall_1000           \tall\t0.5233\n",
    "infAP                 \tall\t0.2994\n",
    "gm_bpref              \tall\t0.4535\n",
    "Rprec_mult_0.20       \tall\t0.6115\n",
    "Rprec_mult_0.40       \tall\t0.5108\n",
    "Rprec_mult_0.60       \tall\t0.4363\n",
    "Rprec_mult_0.80       \tall\t0.3878\n",
    "Rprec_mult_1.00       \tall\t0.3484\n",
    "Rprec_mult_1.20       \tall\t0.3164\n",
    "Rprec_mult_1.40       \tall\t0.2899\n",
    "Rprec_mult_1.60       \tall\t0.2675\n",
    "Rprec_mult_1.80       \tall\t0.2482\n",
    "Rprec_mult_2.00       \tall\t0.2325\n",
    "utility               \tall\t-9397.8667\n",
    "11pt_avg              \tall\t0.3208\n",
    "binG                  \tall\t0.1522\n",
    "G                     \tall\t0.1313\n",
    "ndcg                  \tall\t0.6607\n",
    "ndcg_rel              \tall\t0.5722\n",
    "Rndcg                 \tall\t0.4991\n",
    "ndcg_cut_5            \tall\t0.7299\n",
    "ndcg_cut_10           \tall\t0.7081\n",
    "ndcg_cut_15           \tall\t0.6794\n",
    "ndcg_cut_20           \tall\t0.6650\n",
    "ndcg_cut_30           \tall\t0.6403\n",
    "ndcg_cut_100          \tall\t0.5201\n",
    "ndcg_cut_200          \tall\t0.4570\n",
    "ndcg_cut_500          \tall\t0.4650\n",
    "ndcg_cut_1000         \tall\t0.5236\n",
    "map_cut_5             \tall\t0.0151\n",
    "map_cut_10            \tall\t0.0270\n",
    "map_cut_15            \tall\t0.0380\n",
    "map_cut_20            \tall\t0.0483\n",
    "map_cut_30            \tall\t0.0669\n",
    "map_cut_100           \tall\t0.1449\n",
    "map_cut_200           \tall\t0.1963\n",
    "map_cut_500           \tall\t0.2499\n",
    "map_cut_1000          \tall\t0.2746\n",
    "relative_P_5          \tall\t0.7911\n",
    "relative_P_10         \tall\t0.7689\n",
    "relative_P_15         \tall\t0.7407\n",
    "relative_P_20         \tall\t0.7189\n",
    "relative_P_30         \tall\t0.6911\n",
    "relative_P_100        \tall\t0.5455\n",
    "relative_P_200        \tall\t0.4541\n",
    "relative_P_500        \tall\t0.4353\n",
    "relative_P_1000       \tall\t0.5233\n",
    "success_1             \tall\t0.8000\n",
    "success_5             \tall\t0.9333\n",
    "success_10            \tall\t1.0000\n",
    "set_P                 \tall\t0.0273\n",
    "set_relative_P        \tall\t0.7831\n",
    "set_recall            \tall\t0.7831\n",
    "set_map               \tall\t0.0218\n",
    "set_F                 \tall\t0.0523\n",
    "num_nonrel_judged_ret \tall\t13706\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: empty abstract uid fecx05i8\n",
      "Warning: empty abstract uid lsxg0rrm\n",
      "Warning: empty abstract uid c7cd91pg\n",
      "Warning: empty abstract uid slqjz1wh\n",
      "Warning: empty abstract uid uan6rlvo\n",
      "Warning: empty abstract uid j2wjid4y\n",
      "Warning: empty abstract uid 77mhgade\n",
      "Warning: empty abstract uid mheux808\n",
      "Warning: empty abstract uid sc1biagd\n",
      "Warning: empty abstract uid 3ugtbsx6\n",
      "Warning: empty abstract uid o3vsi9uj\n",
      "Warning: empty abstract uid pn2zi0k1\n",
      "Warning: empty abstract uid f0ny4ur5\n",
      "Warning: empty abstract uid elb3jst0\n",
      "Warning: empty abstract uid tw6968h3\n",
      "Warning: empty abstract uid j0w0yzbj\n",
      "Warning: empty abstract uid slqjz1wh\n",
      "Warning: empty abstract uid 5p3tq0mr\n",
      "Warning: empty abstract uid k1fx7xvb\n",
      "Warning: empty abstract uid 6a6m7ye5\n",
      "Warning: empty abstract uid 6kgliogi\n",
      "Warning: empty abstract uid ai5dyo59\n",
      "Warning: empty abstract uid mah5x0k7\n",
      "Warning: empty abstract uid w67cryfp\n",
      "Warning: empty abstract uid 2sct0c3p\n",
      "Warning: empty abstract uid r9lw01x1\n",
      "Warning: empty abstract uid ogqi5jwv\n",
      "Warning: empty abstract uid cbv3yr77\n",
      "Warning: empty abstract uid 4kxo6fbn\n",
      "Warning: empty abstract uid wb32j7s0\n",
      "Warning: empty abstract uid xn795tcm\n",
      "Warning: empty abstract uid m005jyta\n",
      "Warning: empty abstract uid ke5hxd8o\n",
      "Warning: empty abstract uid 496v31lf\n",
      "Warning: empty abstract uid ei0qno68\n",
      "Warning: empty abstract uid bdlor2wj\n",
      "Warning: empty abstract uid fs983x5g\n",
      "Warning: empty abstract uid ma7cgcvm\n",
      "Warning: empty abstract uid e38itktq\n",
      "Warning: empty abstract uid h5cc0poe\n",
      "Warning: empty abstract uid jo9fgkwl\n",
      "Warning: empty abstract uid xhixiilb\n",
      "Warning: empty abstract uid hj6dobhd\n",
      "Warning: empty abstract uid qnjo4iow\n",
      "Warning: empty abstract uid 2x2ewsp8\n",
      "Warning: empty abstract uid 5s3en8g5\n",
      "Warning: empty abstract uid qhrvi9e7\n",
      "Warning: empty abstract uid mtk7gb2g\n",
      "Warning: empty abstract uid yrbalj5x\n",
      "Warning: empty abstract uid gwf79fj6\n",
      "Warning: empty abstract uid 9mh3ix4m\n",
      "Warning: empty abstract uid 5sgv8ssv\n",
      "Warning: empty abstract uid qigonwc9\n",
      "Warning: empty abstract uid flis1rey\n",
      "Warning: empty abstract uid yfk8e0i4\n",
      "Warning: empty abstract uid 6c08q80z\n",
      "Warning: empty abstract uid 0pfi3huo\n",
      "Warning: empty abstract uid 0ngg5pef\n",
      "Warning: empty abstract uid h139jyk8\n",
      "Warning: empty abstract uid may2tf4u\n",
      "Warning: empty abstract uid rosqr2a5\n",
      "Warning: empty abstract uid knsrfu76\n",
      "Warning: empty abstract uid ynvu5sk5\n",
      "Warning: empty abstract uid vxr50j85\n",
      "Warning: empty abstract uid vbmmznfd\n",
      "Warning: empty abstract uid 780skv92\n",
      "Warning: empty abstract uid vvbepc9g\n",
      "Warning: empty abstract uid aajjsbyy\n",
      "Warning: empty abstract uid zlpz435d\n",
      "Warning: empty abstract uid ss8yikca\n",
      "Warning: empty abstract uid t2uxx9ay\n",
      "Warning: empty abstract uid kf6qorf0\n",
      "Warning: empty abstract uid yqbi7zsv\n",
      "Warning: empty abstract uid hwfr5v1i\n",
      "Warning: empty abstract uid 27x49fxg\n",
      "Warning: empty abstract uid tug733xj\n",
      "Warning: empty abstract uid 281pj442\n",
      "Warning: empty abstract uid vha2fj9s\n",
      "Warning: empty abstract uid ui15p68e\n",
      "Warning: empty abstract uid 3a8a9ecs\n",
      "Warning: empty abstract uid 2e2iwi3k\n",
      "Warning: empty abstract uid znl466c4\n",
      "Warning: empty abstract uid eksiyh9q\n",
      "Warning: empty abstract uid 8uhszgc9\n",
      "Warning: empty abstract uid 57k6dw89\n",
      "Warning: empty abstract uid cn58n6sc\n",
      "Warning: empty abstract uid qbr4wfrj\n",
      "Warning: empty abstract uid hqyungju\n",
      "Warning: empty abstract uid fj5mkmgv\n",
      "Warning: empty abstract uid ifn3jij5\n",
      "Warning: empty abstract uid 2nnq5uf0\n",
      "Warning: empty abstract uid 0wxf08hc\n",
      "Warning: empty abstract uid 4rl39on6\n",
      "Warning: empty abstract uid r3548qne\n",
      "Warning: empty abstract uid urrkou60\n",
      "Warning: empty abstract uid mcyvnxji\n",
      "Warning: empty abstract uid uajq90go\n",
      "Warning: empty abstract uid q3xyws48\n",
      "Warning: empty abstract uid do5t1gmt\n",
      "Warning: empty abstract uid c54cx1p1\n",
      "Warning: empty abstract uid rfo9m1qw\n",
      "Warning: empty abstract uid hzd65sqy\n",
      "Warning: empty abstract uid cme34stj\n",
      "Warning: empty abstract uid vq3nf78d\n",
      "Warning: empty abstract uid 9hq8xdhi\n",
      "Warning: empty abstract uid a4vo5bvn\n",
      "Warning: empty abstract uid 674gazsx\n",
      "Warning: empty abstract uid trt7droe\n",
      "Warning: empty abstract uid i8gz74n3\n",
      "Warning: empty abstract uid s5q6pxhh\n",
      "Warning: empty abstract uid 6sh2kasa\n",
      "Warning: empty abstract uid ly3hrfml\n",
      "Warning: empty abstract uid xl9wh4wv\n",
      "Warning: empty abstract uid dy9hchm8\n",
      "Warning: empty abstract uid 7w6dxh0c\n",
      "Warning: empty abstract uid 8w5on84o\n",
      "Warning: empty abstract uid 84vuo4d3\n",
      "Warning: empty abstract uid qa2phm5l\n",
      "Warning: empty abstract uid sf9jjz6o\n",
      "Warning: empty abstract uid k6j789p9\n",
      "Warning: empty abstract uid zy1byu51\n",
      "Warning: empty abstract uid b18l7je9\n",
      "Warning: empty abstract uid gj42zytf\n",
      "Warning: empty abstract uid 15y3s2ew\n",
      "Warning: empty abstract uid z19xdrxu\n",
      "Warning: empty abstract uid woylyufz\n",
      "Warning: empty abstract uid at0tm7tp\n",
      "Warning: empty abstract uid es1jkxwt\n",
      "Warning: empty abstract uid ungqzaqi\n",
      "Warning: empty abstract uid rm8aykgr\n",
      "Warning: empty abstract uid 7maie83l\n",
      "Warning: empty abstract uid veckwubt\n",
      "Warning: empty abstract uid ac1x4pcf\n",
      "Warning: empty abstract uid 97mg41jl\n",
      "Warning: empty abstract uid zpib6e2z\n",
      "Warning: empty abstract uid fxzqdp1o\n",
      "Warning: empty abstract uid 6336bqi9\n",
      "Warning: empty abstract uid 4lswuyjk\n",
      "Warning: empty abstract uid 1uyly36n\n",
      "Warning: empty abstract uid il8j9jsw\n",
      "Warning: empty abstract uid nclq3sm0\n",
      "Warning: empty abstract uid 9keet8ih\n",
      "Warning: empty abstract uid o60ldhzc\n",
      "Warning: empty abstract uid 1h2t9c5g\n",
      "Warning: empty abstract uid vjgrq22k\n",
      "Warning: empty abstract uid i89iwb10\n",
      "Warning: empty abstract uid ynvw6i4t\n",
      "Warning: empty abstract uid 886xc1n3\n",
      "Warning: empty abstract uid awp1nck0\n",
      "Warning: empty abstract uid akc5t2b8\n",
      "Warning: empty abstract uid gz6gpkfs\n",
      "Warning: empty abstract uid 9jakiead\n",
      "Warning: empty abstract uid c2c6y0qr\n",
      "Warning: empty abstract uid glfhgz35\n",
      "Warning: empty abstract uid i5v5xcv1\n",
      "Warning: empty abstract uid ccy9vqcb\n",
      "Warning: empty abstract uid ya6zkze9\n",
      "Warning: empty abstract uid 1fbgpvm9\n",
      "Warning: empty abstract uid j5j2jarw\n",
      "Warning: empty abstract uid r8n9g7mv\n",
      "Warning: empty abstract uid g0ndggwc\n",
      "Warning: empty abstract uid cv3op3bb\n",
      "Warning: empty abstract uid nuaq50vi\n",
      "Warning: empty abstract uid tloq4oq7\n",
      "Warning: empty abstract uid dk1rk908\n",
      "Warning: empty abstract uid zx767txr\n",
      "Warning: empty abstract uid q4vwasb3\n",
      "Warning: empty abstract uid 0h8b051i\n",
      "Warning: empty abstract uid y42o6exe\n",
      "Warning: empty abstract uid td4ljqtc\n",
      "Warning: empty abstract uid lcqn2fk1\n",
      "Warning: empty abstract uid 063yv6mm\n",
      "Warning: empty abstract uid iuao03mv\n",
      "Warning: empty abstract uid skwu025v\n",
      "Warning: empty abstract uid rco7gbtv\n",
      "Warning: empty abstract uid re902ol2\n",
      "Warning: empty abstract uid usqjkj89\n",
      "Warning: empty abstract uid 3gpkb8nm\n",
      "Warning: empty abstract uid pvl44u4d\n",
      "Warning: empty abstract uid 3b2e8x8z\n",
      "Warning: empty abstract uid c8nxqw11\n",
      "Warning: empty abstract uid enubfc2k\n",
      "Warning: empty abstract uid qnm4zkme\n",
      "Warning: empty abstract uid y3y4z0vq\n",
      "Warning: empty abstract uid jni56jzf\n",
      "Warning: empty abstract uid 7ctk4lsx\n",
      "Warning: empty abstract uid xnce010o\n",
      "Warning: empty abstract uid rir2cxns\n",
      "Warning: empty abstract uid cn1z7aen\n",
      "Warning: empty abstract uid in6yuntn\n",
      "Warning: empty abstract uid apaah4xy\n"
     ]
    }
   ],
   "source": [
    "# Read Anserini Baseline\n",
    "\n",
    "with open(PATH_TO_ANSERINI_RUN, \"r\") as f:\n",
    "    anserini_run = f.readlines()\n",
    "    \n",
    "set_of_top_k_uid_all_topics = set() # We won't get all abstracts because there are too many.\n",
    "\n",
    "for line in anserini_run:\n",
    "    qid, _, uid, rank, _, _ = line.strip().split()\n",
    "    if int(rank) <= 200:\n",
    "        set_of_top_k_uid_all_topics.add(uid)\n",
    "\n",
    "# Read topics\n",
    "\n",
    "topics = read_topics(PATH_TO_TOPICS)\n",
    "\n",
    "# Read Abstract\n",
    "\n",
    "abstracts_dict = get_abstract(PATH_TO_CORD_19_DATA, set_of_top_k_uid_all_topics)\n",
    "\n",
    "\n",
    "# load SciBert (alternative: monologg/biobert_v1.1_pubmed)\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', do_lower_case=False)\n",
    "model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reranking using SciBERT + Words-level Matches\n",
    "\n",
    "\n",
    "What we are doing is - given Anserini Run on abstract, use SciBERT to rerank first K abstracts, and evaluate using TREC metric as usual. \n",
    "\n",
    "In short, we use SciBERT to encode the abstracts and queries into vectors of size [ num_of_tokens, 768]. If the sentence is longer than maximum size of BERT, use slicing windows. We use the vectors of the last hidden states of BERT model.\n",
    "\n",
    "\n",
    "Once both abstracts and queries are encoded, we do token level cosine similarity to score the relevant of paragraph and query with the highly matched words.\n",
    "\n",
    "This approach is inspired by [Notebook](https://github.com/castorini/anserini-notebooks/blob/master/Pyserini%2BSciBERT_on_COVID_19_Demo.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 510\n",
    "\n",
    "def extract_scibert(text, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Encode text to vectors\n",
    "    @text string to be encoded\n",
    "    @tokenizer BertTokenizer object\n",
    "    @model BertModel object\n",
    "    @return tensor of size [num_tokens, 768] (last hidden state of BERT)\n",
    "    \"\"\"\n",
    "    \n",
    "    text_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])\n",
    "    text_words = tokenizer.convert_ids_to_tokens(text_ids[0])[1:-1]\n",
    "\n",
    "    n_chunks = int(numpy.ceil(float(text_ids.size(1))/CHUNK_SIZE))\n",
    "    states = []\n",
    "    \n",
    "    for ci in range(n_chunks):\n",
    "        text_ids_ = text_ids[0, 1+ci*CHUNK_SIZE:1+(ci+1)*CHUNK_SIZE]  \n",
    "        text_ids_ = torch.cat([text_ids[0, 0].unsqueeze(0), text_ids_])\n",
    "        if text_ids_[-1] != text_ids[0, -1]: # their code is wrong\n",
    "\n",
    "            text_ids_ = torch.cat([text_ids_, text_ids[0,-1].unsqueeze(0)])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state = model(text_ids_.unsqueeze(0))[0] # last hidden states\n",
    "            state = state[:, 1:-1, :]\n",
    "        states.append(state)\n",
    "\n",
    "    state = torch.cat(states, axis=1)\n",
    "    # return text_ids, text_words, state[0]\n",
    "    return state[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_abstract_query_narrative_and_save(topics, abstracts_dict, extract_scibert, tokenizer, model, path_to_output, start_at=0, **kwargs):\n",
    "    \"\"\"\n",
    "    Encode topics and abstracts using given encoding function, save it to path_to_output\n",
    "    @topics\n",
    "    @abstracts_dict\n",
    "    @extract_scibert BERT encoding function, \n",
    "        input: text, tokenizer, model **kwargs, \n",
    "        output  tensor of size [num, 768]\n",
    "    @tokenizer to pass to extract_scibert\n",
    "    @model to pass to extract_scibert\n",
    "    @path_to_output output filename\n",
    "    @start_at in case we want to skip first k abstracts\n",
    "    \"\"\"\n",
    "    \n",
    "    # encode abstract\n",
    "    encoded_abstract = dict()\n",
    "\n",
    "    for i, (k, text) in tqdm(enumerate(abstracts_dict.items())):\n",
    "        if i > start_at:\n",
    "            encoded_abstract[text] = extract_scibert(text, tokenizer, model, **kwargs)\n",
    "\n",
    "    # encode topics\n",
    "    encoded_queries = dict()\n",
    "    encoded_naratives = dict()\n",
    "\n",
    "    for topic in topics:\n",
    "        encoded_queries[topic[\"query\"]] = extract_scibert(topic[\"query\"], tokenizer, model)\n",
    "        encoded_naratives[topic[\"narrative\"]] = extract_scibert(topic[\"narrative\"], tokenizer, model)\n",
    "\n",
    "    # save for future use\n",
    "    bert_vectors = {\n",
    "            \"abstract\": encoded_abstract, \n",
    "            \"query\": encoded_queries, \n",
    "            \"narrative\": encoded_naratives\n",
    "    }\n",
    "\n",
    "    with open(\"../tmp/\" + path_to_output, \"wb\") as f:\n",
    "        pickle.dump(bert_vectors, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encode_abstract_query_narrative_and_save(\n",
    "    topics, \n",
    "    abstracts_dict, \n",
    "    extract_scibert, \n",
    "    tokenizer, \n",
    "    model, \n",
    "    \"scibert.pkl\"\n",
    ")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "with open(\"../tmp/scibert.pkl\", \"rb\") as f:\n",
    "    bert_vectors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_match(state1, state2):\n",
    "    state1 = state1 / torch.sqrt((state1 ** 2).sum(1, keepdims=True))\n",
    "    state2 = state2 / torch.sqrt((state2 ** 2).sum(1, keepdims=True))\n",
    "    sim = (state1.unsqueeze(1) * state2.unsqueeze(0)).sum(-1)\n",
    "    return sim\n",
    "\n",
    "def rerank(topics, anserini_run, abstracts_dict, top_k, path_to_reranked_run, topics_field, bert_vectors):\n",
    "    \"\"\"\n",
    "    Rerank the original run and save the reranked run in path_to_reranked_run\n",
    "    @topics list of dict where dict represents each topic\n",
    "    [\n",
    "        {\n",
    "            'number': '1', \n",
    "            'query': 'coronavirus origin', \n",
    "            'question': 'what is the origin of COVID-19',\n",
    "            'narrative': \"seeking range of information about ...\"\n",
    "         },\n",
    "         ...\n",
    "    ]\n",
    "    @abstracts_dict dict where key=uid and value=abstract\n",
    "    @anserini_run list of string run in anserini_run\n",
    "    @top_k to rerank, the rest will remain same\n",
    "    @path_to_reranked_run path to save the reranked run\n",
    "    @topics_field query or narrative\n",
    "    @bert_vectors dict k=query,abstract,narrative v=vectors\n",
    "    \"\"\"\n",
    "    rerank = defaultdict(list)  # first k hits\n",
    "    keeprank = defaultdict(list) # k+1 to 1000 hits\n",
    "    query_dict = {e[\"number\"]: e[topics_field] for e in topics}  # choose whether to use query or narrative\n",
    "    encoded_queries = bert_vectors[topics_field] # choose whether to use query or narrative\n",
    "    encoded_abstract = bert_vectors[\"abstract\"]\n",
    "\n",
    "    # calculate similarity\n",
    "    for line in anserini_run:\n",
    "        qid, _, uid, j, score, _ = line.strip().split()\n",
    "        if len(rerank[qid]) < top_k:\n",
    "            if not abstracts_dict[uid]:\n",
    "                continue # Some uid don't have abstract. But why they show up in Anserini run?\n",
    "\n",
    "            _, _, enc_abs = encoded_abstract[abstracts_dict[uid]]\n",
    "            _, _, enc_query = encoded_queries[query_dict[qid]]\n",
    "            sim = cross_match(enc_query, enc_abs)\n",
    "\n",
    "            rel_score = torch.max(sim).item()\n",
    "            rerank[qid].append([uid, rel_score])\n",
    "\n",
    "        elif len(rerank[qid]) >= top_k and len(keeprank[qid]) < 1000 - top_k: \n",
    "            keeprank[qid].append([uid, score, j])\n",
    "\n",
    "\n",
    "    # create reranked run and save to path_to_reranked_run\n",
    "    template = \"{} Q0 {} {} {} anserini_scibert\"\n",
    "    run = list()\n",
    "\n",
    "    for qid in rerank:\n",
    "        rank = 1\n",
    "        for uid, score in sorted(rerank[qid], key=lambda x:-x[1]):\n",
    "            run.append(template.format(qid, uid, rank, score + 10))\n",
    "            rank += 1\n",
    "\n",
    "        for uid, score, j in keeprank[qid]:\n",
    "            run.append(template.format(qid, uid, rank, score))\n",
    "            rank += 1\n",
    "            \n",
    "        assert rank == 1001 # if no bugs, each topic will have at most 1000 uid (can be less if original run has less)\n",
    "\n",
    "    with open(\"../tmp/\" + path_to_reranked_run, \"w\") as f:\n",
    "        f.write(\"\\n\".join(run))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_reranked_run = \"scibert_sim_rerank.txt\"\n",
    "topics_field = \"narrative\" # query or narrative\n",
    "rerank(topics, anserini_run, abstracts_dict, 100, path_to_reranked_run, topics_field, bert_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ret = os.system(PATH_TO_TREC + \" -c -m all_trec ../tmp/qrels-covid_d4_j0.5-4.txt  ../tmp/scibert_sim_rerank_on_narative.txt > ../tmp/out.txt\")\n",
    "if ret == 0:\n",
    "    with open(\"../tmp/out.txt\", \"r\") as f:\n",
    "        print(f.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result after reranking**\n",
    "\n",
    "\n",
    "It turns out that this approach doesn't improve the reranking.\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "runid                 \tall\tanserini_scibert\n",
    "num_q                 \tall\t45\n",
    "num_ret               \tall\t44955\n",
    "num_rel               \tall\t15765\n",
    "num_rel_ret           \tall\t7678\n",
    "map                   \tall\t0.1947\n",
    "gm_map                \tall\t0.1564\n",
    "Rprec                 \tall\t0.2981\n",
    "bpref                 \tall\t0.4178\n",
    "recip_rank            \tall\t0.7022\n",
    "iprec_at_recall_0.00  \tall\t0.8272\n",
    "iprec_at_recall_0.10  \tall\t0.4782\n",
    "iprec_at_recall_0.20  \tall\t0.3947\n",
    "iprec_at_recall_0.30  \tall\t0.3183\n",
    "iprec_at_recall_0.40  \tall\t0.2316\n",
    "iprec_at_recall_0.50  \tall\t0.1332\n",
    "iprec_at_recall_0.60  \tall\t0.0942\n",
    "iprec_at_recall_0.70  \tall\t0.0484\n",
    "iprec_at_recall_0.80  \tall\t0.0317\n",
    "iprec_at_recall_0.90  \tall\t0.0053\n",
    "iprec_at_recall_1.00  \tall\t0.0000\n",
    "P_5                   \tall\t0.5244\n",
    "P_10                  \tall\t0.5378\n",
    "P_15                  \tall\t0.5126\n",
    "P_20                  \tall\t0.4933\n",
    "P_30                  \tall\t0.4778\n",
    "P_100                 \tall\t0.4300\n",
    "P_200                 \tall\t0.3614\n",
    "P_500                 \tall\t0.2406\n",
    "P_1000                \tall\t0.1706\n",
    "recall_5              \tall\t0.0091\n",
    "recall_10             \tall\t0.0177\n",
    "recall_15             \tall\t0.0258\n",
    "recall_20             \tall\t0.0333\n",
    "recall_30             \tall\t0.0473\n",
    "recall_100            \tall\t0.1445\n",
    "recall_200            \tall\t0.2385\n",
    "recall_500            \tall\t0.3819\n",
    "recall_1000           \tall\t0.5172\n",
    "infAP                 \tall\t0.1947\n",
    "gm_bpref              \tall\t0.3567\n",
    "Rprec_mult_0.20       \tall\t0.4542\n",
    "Rprec_mult_0.40       \tall\t0.3903\n",
    "Rprec_mult_0.60       \tall\t0.3560\n",
    "Rprec_mult_0.80       \tall\t0.3260\n",
    "Rprec_mult_1.00       \tall\t0.2981\n",
    "Rprec_mult_1.20       \tall\t0.2731\n",
    "Rprec_mult_1.40       \tall\t0.2512\n",
    "Rprec_mult_1.60       \tall\t0.2327\n",
    "Rprec_mult_1.80       \tall\t0.2173\n",
    "Rprec_mult_2.00       \tall\t0.2044\n",
    "utility               \tall\t-657.7556\n",
    "11pt_avg              \tall\t0.2330\n",
    "binG                  \tall\t0.0900\n",
    "G                     \tall\t0.0810\n",
    "ndcg                  \tall\t0.4718\n",
    "ndcg_rel              \tall\t0.4235\n",
    "Rndcg                 \tall\t0.3752\n",
    "ndcg_cut_5            \tall\t0.4589\n",
    "ndcg_cut_10           \tall\t0.4637\n",
    "ndcg_cut_15           \tall\t0.4468\n",
    "ndcg_cut_20           \tall\t0.4326\n",
    "ndcg_cut_30           \tall\t0.4216\n",
    "ndcg_cut_100          \tall\t0.3865\n",
    "ndcg_cut_200          \tall\t0.3628\n",
    "ndcg_cut_500          \tall\t0.3883\n",
    "ndcg_cut_1000         \tall\t0.4718\n",
    "map_cut_5             \tall\t0.0072\n",
    "map_cut_10            \tall\t0.0128\n",
    "map_cut_15            \tall\t0.0172\n",
    "map_cut_20            \tall\t0.0214\n",
    "map_cut_30            \tall\t0.0293\n",
    "map_cut_100           \tall\t0.0768\n",
    "map_cut_200           \tall\t0.1181\n",
    "map_cut_500           \tall\t0.1640\n",
    "map_cut_1000          \tall\t0.1947\n",
    "relative_P_5          \tall\t0.5244\n",
    "relative_P_10         \tall\t0.5378\n",
    "relative_P_15         \tall\t0.5126\n",
    "relative_P_20         \tall\t0.4933\n",
    "relative_P_30         \tall\t0.4778\n",
    "relative_P_100        \tall\t0.4320\n",
    "relative_P_200        \tall\t0.3882\n",
    "relative_P_500        \tall\t0.3934\n",
    "relative_P_1000       \tall\t0.5172\n",
    "success_1             \tall\t0.5556\n",
    "success_5             \tall\t0.8667\n",
    "success_10            \tall\t0.9556\n",
    "set_P                 \tall\t0.1708\n",
    "set_relative_P        \tall\t0.5172\n",
    "set_recall            \tall\t0.5172\n",
    "set_map               \tall\t0.0937\n",
    "set_F                 \tall\t0.2449\n",
    "num_nonrel_judged_ret \tall\t5724\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reranking using SciBERT + Phrase-level Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_scibert_phrase(text, tokenizer, model, pool, window_size = 100, overlap_size = 20):\n",
    "    \"\"\"\n",
    "    Encode text to vectors using slicing windows and overlap. For example,\n",
    "    text=\"I love Deep Learning and Information Retrieval\", window_size=5, overlap_size=2\n",
    "    BERT will encode:\n",
    "    \"I love Deep Learning and\"\n",
    "    \"Learning and Information Retrieval\"\n",
    "    The tokens in each phrase are pooled using method defined by pool.\n",
    "    \n",
    "    @text string to be encoded\n",
    "    @tokenizer BertTokenizer object\n",
    "    @model BertModel object\n",
    "    @pool \"cls\" or \"sum\". If class, use [CLS] vector. If sum, sum all token vectors except [CLS] and [SEP]. \n",
    "    see https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel\n",
    "    @return tensor of size [num_phrases, 768]\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    num_phrases = int(numpy.ceil(max(len(text.split()) - window_size, 0) / (window_size - overlap_size))) + 1\n",
    "\n",
    "    vectors = []\n",
    "    for batch in range(num_phrases):\n",
    "        start = batch * (window_size - overlap_size)\n",
    "        end = start + window_size\n",
    "        s = \" \".join(text.split()[start:end])\n",
    "\n",
    "        # convert to vectors\n",
    "        inputs = tokenizer(s, return_tensors=\"pt\")\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        if pool == \"cls\":\n",
    "            output = outputs[1]\n",
    "        elif pool == \"sum\":\n",
    "            output = outputs[0][0,1:-1,:].sum(axis=0).reshape(1,-1)\n",
    "        else:\n",
    "            print(\"Invalid Pool Method. Must be 'cls' or 'sum'.\")\n",
    "\n",
    "        vectors.append(output)\n",
    "    \n",
    "    output_vectors = torch.cat(vectors, axis=0)\n",
    "    return output_vectors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1139it [05:49,  3.38it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "encode_abstract_query_narrative_and_save(\n",
    "    topics, \n",
    "    abstracts_dict, \n",
    "    extract_scibert_phrase, \n",
    "    tokenizer, \n",
    "    model, \n",
    "    \"scibert_phrase_cls_w_100_o_20.pkl\",\n",
    "    pool=\"cls\"\n",
    "\n",
    ")\n",
    "    \n",
    "    \n",
    "\n",
    "encode_abstract_query_narrative_and_save(\n",
    "    topics, \n",
    "    abstracts_dict, \n",
    "    extract_scibert_phrase, \n",
    "    tokenizer, \n",
    "    model, \n",
    "    \"scibert_phrase_sum_w_100_o_20.pkl\",\n",
    "    pool=\"sum\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reranking with BertForNextSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
